# ai_security_labs

# 🛡️ AI Security Lab Series – 12 Weeks of GenAI Defense

Welcome to my hands-on AI Security Lab series — a 12-week journey into building and breaking LLM applications. This repo documents my exploration of securing GenAI systems using Python, Flask, LangChain, FAISS, OpenAI API, and cloud-native tools.

🔐 **What You'll Find Here:**
- Real-world threat modeling for LLMs
- Prompt injection attacks + custom firewalls
- Secure RAG pipelines (Retrieval-Augmented Generation)
- Red teaming, abuse detection, and PII filtering
- Cloud deployment, logging, and access control

📁 **Lab Breakdown:**
| Week | Project | Focus |
|------|---------|-------|
| 1 | Basic Chatbot | LLM API Integration |
| 2 | Prompt Injection Playground | Adversarial Prompt Testing |
| 3 | Abuse Detection System | Prompt Logging + Alerting |
| 4 | RAG App | Retrieval-Augmented Generation |
| 5 | Secure the RAG | Auth, Rate Limiting |
| 6 | LLM Red Team Lab | Attack Simulation |
| 7 | Secure Deployment | IAM, Logging, Gateway |
| 8 | Shadow Prompt Scanner | Detection Rules |
| 9 | Poisoned Knowledge Base | Integrity Testing |
| 10 | Secure Prompt Gateway | API Wrapping + Filtering |
| 11 | PII Defense | Regex Redaction |
| 12 | Portfolio Polish | README, GitHub, LinkedIn Launch |

🧠 **Why I Built This:**  
Because the future of AI security won’t be PowerPoints — it’ll be pipelines, payloads, and practical defense.

---

👩🏽‍💻 Made by Andrea E. 
💻 Python • LLMs • Cloud • AppSec  
🔗 LinkedIn: [linkedin.com/in/edwardsat](https://www.linkedin.com/in/edwardsat/)  
📂 Resume: coming soon!
