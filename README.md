# ai_security_labs

# ğŸ›¡ï¸ AI Security Lab Series â€“ 12 Weeks of GenAI Defense

Welcome to my hands-on AI Security Lab series â€” a 12-week journey into building and breaking LLM applications. This repo documents my exploration of securing GenAI systems using Python, Flask, LangChain, FAISS, OpenAI API, and cloud-native tools.

ğŸ” **What You'll Find Here:**
- Real-world threat modeling for LLMs
- Prompt injection attacks + custom firewalls
- Secure RAG pipelines (Retrieval-Augmented Generation)
- Red teaming, abuse detection, and PII filtering
- Cloud deployment, logging, and access control

ğŸ“ **Lab Breakdown:**
| Week | Project | Focus |
|------|---------|-------|
| 1 | Basic Chatbot | LLM API Integration |
| 2 | Prompt Injection Playground | Adversarial Prompt Testing |
| 3 | Abuse Detection System | Prompt Logging + Alerting |
| 4 | RAG App | Retrieval-Augmented Generation |
| 5 | Secure the RAG | Auth, Rate Limiting |
| 6 | LLM Red Team Lab | Attack Simulation |
| 7 | Secure Deployment | IAM, Logging, Gateway |
| 8 | Shadow Prompt Scanner | Detection Rules |
| 9 | Poisoned Knowledge Base | Integrity Testing |
| 10 | Secure Prompt Gateway | API Wrapping + Filtering |
| 11 | PII Defense | Regex Redaction |
| 12 | Portfolio Polish | README, GitHub, LinkedIn Launch |

ğŸ§  **Why I Built This:**  
Because the future of AI security wonâ€™t be PowerPoints â€” itâ€™ll be pipelines, payloads, and practical defense.

---

ğŸ‘©ğŸ½â€ğŸ’» Made by Andrea E. 
ğŸ’» Python â€¢ LLMs â€¢ Cloud â€¢ AppSec  
ğŸ”— LinkedIn: [linkedin.com/in/edwardsat](https://www.linkedin.com/in/edwardsat/)  
ğŸ“‚ Resume: coming soon!
